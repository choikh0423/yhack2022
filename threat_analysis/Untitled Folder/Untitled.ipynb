{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bea320f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: facebook-sdk in c:\\users\\seung\\anaconda3\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: requests in c:\\users\\seung\\anaconda3\\lib\\site-packages (from facebook-sdk) (2.25.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\seung\\anaconda3\\lib\\site-packages (from requests->facebook-sdk) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\seung\\anaconda3\\lib\\site-packages (from requests->facebook-sdk) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\seung\\anaconda3\\lib\\site-packages (from requests->facebook-sdk) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\seung\\anaconda3\\lib\\site-packages (from requests->facebook-sdk) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install facebook-sdk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "986db46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, facebook, json, time\n",
    "\n",
    "# Getting User Token for Facebook Graph API\n",
    "user_token = 'EAAKh8qpAhWIBAO0ebs5v95VjWoW9LOVhY1uIzhbl5Erfoj0mMHb7M22B3gxNEUX3LaMo6fKqslRRqMOOSPTxx37v4Qo83sV7egB7jBe3aWODZBZA7cNbNvZBqtnsnmC7EnuciYRxgLiPs4gyEa1duZAQrDhMuK529aUZArcthlyO2eYz7w3cg5yjluJPcz2yPSRzvKzT89BROjMb8cDenPGJZB9QIZBSlQWcZC7uRuB6ZCyy5cN7my5lGqtaogW2TvW0ZD'\n",
    "graph = facebook.GraphAPI(user_token)\n",
    "profile = graph.get_object(\"me\", fields=\"posts\")\n",
    "posts_dict = profile['posts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "717d8a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Function retrieves all the posts from one page\n",
    "def get_page_posts(posts_dict):\n",
    "    posts = posts_dict['data']\n",
    "    post_infos = []\n",
    "\n",
    "    for post in posts:\n",
    "        try:\n",
    "            created_date = post['created_time'][:10]\n",
    "            message = post['message']\n",
    "            url = \"https://www.facebook.com/\" + post['id']\n",
    "        except:\n",
    "            message = None\n",
    "\n",
    "\n",
    "        if message:\n",
    "            comb = [created_date, \"Facebook\", \"Post\", url, message]\n",
    "            post_infos.append(comb)\n",
    "        \n",
    "\n",
    "    return post_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed11822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Function returns all the posts from all the pages\n",
    "def get_all_posts(posts_dict):\n",
    "    pages = posts_dict['paging']\n",
    "    data = posts_dict['data']\n",
    "    new_post_dict = posts_dict\n",
    "    all_posts =[]\n",
    "\n",
    "\n",
    "    while data != []:\n",
    "        page_posts = get_page_posts(new_post_dict)\n",
    "        all_posts.extend(page_posts)\n",
    "\n",
    "        next_page = pages['previous']\n",
    "\n",
    "        next_request = requests.get(next_page)\n",
    "\n",
    "        new_post_dict = next_request.json()\n",
    "        data = new_post_dict['data']\n",
    "\n",
    "    return all_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e914585d",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = get_all_posts(posts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "05be62b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['2022-04-10', 'Facebook', 'Post', 'https://www.facebook.com/107163565298907_107930128555584', 'cool story bro. In what chapter do you shut the fuck up?'], ['2022-04-10', 'Facebook', 'Post', 'https://www.facebook.com/107163565298907_107930088555588', 'turn your wounds into wisdom'], ['2022-04-10', 'Facebook', 'Post', 'https://www.facebook.com/107163565298907_107929985222265', 'our love was lost. Your heart turned cold'], ['2022-04-10', 'Facebook', 'Post', 'https://www.facebook.com/107163565298907_107929815222282', 'what a job.'], ['2022-04-10', 'Facebook', 'Post', 'https://www.facebook.com/107163565298907_107929778555619', \"I love this Pizza, who's the chef?\"], ['2022-04-10', 'Facebook', 'Post', 'https://www.facebook.com/107163565298907_107929668555630', 'yo Check THIS Out!! Hit me up!! 010-XXXX-XXXX'], ['2022-04-10', 'Facebook', 'Post', 'https://www.facebook.com/107163565298907_107929545222309', \"have you heard of Elon Musk's Neural link? That is fucking awesome!!\"], ['2022-04-10', 'Facebook', 'Post', 'https://www.facebook.com/107163565298907_107929261889004', 'thanks for you help. I really appreciate it'], ['2022-04-10', 'Facebook', 'Post', 'https://www.facebook.com/107163565298907_107929198555677', 'I love dogs. They are very cute.'], ['2022-04-10', 'Facebook', 'Post', 'https://www.facebook.com/107163565298907_107929145222349', \"I'm broke. Would anyone raise a fund for me? Call me. 010-xxxx-xxxx\"], ['2022-04-10', 'Facebook', 'Post', 'https://www.facebook.com/107163565298907_107929025222361', 'my life has no hope. I should just go die'], ['2022-04-10', 'Facebook', 'Post', 'https://www.facebook.com/107163565298907_107928965222367', \"I hate my mom's beef stew it's fucking disgusting\"], ['2022-04-10', 'Facebook', 'Post', 'https://www.facebook.com/107163565298907_107927911889139', 'This account is a test account for app development.']]\n"
     ]
    }
   ],
   "source": [
    "print(posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b31e30e",
   "metadata": {},
   "source": [
    "### \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "544a5f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\seung\\anaconda3\\lib\\site-packages (3.6.1)\n",
      "Requirement already satisfied: click in c:\\users\\seung\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in c:\\users\\seung\\anaconda3\\lib\\site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\seung\\anaconda3\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\seung\\anaconda3\\lib\\site-packages (from nltk) (4.59.0)\n",
      "Requirement already satisfied: better-profanity in c:\\users\\seung\\anaconda3\\lib\\site-packages (0.7.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\seung\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# installing requirements\n",
    "!pip install nltk\n",
    "!pip install better-profanity\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from better_profanity import profanity\n",
    "import pandas as pd\n",
    "\n",
    "# loading train data\n",
    "review_df = pd.read_csv(r'C:\\Users\\seung\\Code\\yhack2022\\Untitled Folder\\review_data.txt', header = 0, sep = \"\\t\", quoting = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0954fea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# porfanity and personal information filter\n",
    "personal_info=['010-xxxx-xxxx']\n",
    "\n",
    "# profanity_counter = 0\n",
    "# personal_information_counter = 0\n",
    "\n",
    "def checking(words_list):\n",
    "    filter_list=[]\n",
    "    profanity_counter = 0\n",
    "    personal_information_counter = 0\n",
    "    \n",
    "    content = word_tokenize(words_list)\n",
    "\n",
    "    for element in content:\n",
    "        \n",
    "        if profanity.contains_profanity(element) == True:\n",
    "            profanity_counter += 1\n",
    "        if element in personal_info:\n",
    "            personal_information_counter += 1\n",
    "    if profanity_counter > 0 :\n",
    "        filter_list.append(\"profanity\")\n",
    "\n",
    "    if personal_information_counter > 0 :\n",
    "        filter_list.append(\"personal information\")\n",
    "    \n",
    "    return filter_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "848acaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_list2=[]\n",
    "for k in range(len(posts)):\n",
    "    filter_list2.append(checking(posts[k][-1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f3ce7762",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-5dd1e6bef520>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# 파이썬의 re 모듈을 이용해 영어 문자열이 아닌 문자는 모두 공백으로 변환\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mreview_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'review'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreview_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'review'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"[^a-zA-Z]\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   4136\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4137\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4138\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4140\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-38-5dd1e6bef520>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# 파이썬의 re 모듈을 이용해 영어 문자열이 아닌 문자는 모두 공백으로 변환\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mreview_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'review'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreview_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'review'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"[^a-zA-Z]\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\re.py\u001b[0m in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[1;32m--> 210\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    " #unstable mood filter\n",
    "import re\n",
    "\n",
    "# <br> html 태그는 replace 함수로 공백으로 변환\n",
    "review_df['review'] = review_df['review'].str.replace('<br />', ' ')\n",
    "\n",
    "# 파이썬의 re 모듈을 이용해 영어 문자열이 아닌 문자는 모두 공백으로 변환\n",
    "review_df['review'] = review_df['review'].apply(lambda x : re.sub(\"[^a-zA-Z]\", \" \", x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91e08144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20044, 1), (5011, 1))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결정 값 클래스인 sentiment 칼럼을 별도로 추출\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# 학습 데이터에서 sentiment만 추출\n",
    "class_df = review_df['sentiment']\n",
    "# 학습 데이터에서 review만 추출\n",
    "feature_df = review_df.drop(['id','sentiment'], axis=1, inplace=False)\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(feature_df, class_df, test_size=0.2, random_state=156,shuffle=True)\n",
    "X_train.shape, X_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cecc509e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-40450c3bad35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# Pipeline 객체를 이용하여 fit(), predict()로 학습/예측 수행. predict_proba()는 roc_auc때문에 수행.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'review'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'review'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mpred_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'review'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    339\u001b[0m         \"\"\"\n\u001b[0;32m    340\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[0;32m    343\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    301\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m             \u001b[1;31m# Fit or load from cache the current transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 303\u001b[1;33m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[0;32m    304\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Pipeline'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    752\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit_transform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1201\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1203\u001b[1;33m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[0;32m   1204\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[0;32m   1205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1114\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1115\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1116\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1117\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0manalyzer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m             raise ValueError(\"np.nan is an invalid document, expected byte or \"\n\u001b[0m\u001b[0;32m    218\u001b[0m                              \"unicode string.\")\n\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# 스톱 워드는 English, filtering, ngram은 (1,2)로 설정해 CountVectorization수행. \n",
    "# LogisticRegression의 C는 10으로 설정. \n",
    "pipeline = Pipeline([\n",
    "    ('cnt_vect', CountVectorizer(stop_words='english', ngram_range=(1,2) )),\n",
    "    ('lr_clf', LogisticRegression(C=2, max_iter = 100))]) # C 값이 작으면 훈련을 덜 복잡 ( 강한 규제)\n",
    "\n",
    "# Pipeline 객체를 이용하여 fit(), predict()로 학습/예측 수행. predict_proba()는 roc_auc때문에 수행.  \n",
    "pipeline.fit(X_train['review'], y_train)\n",
    "pred =pipeline.predict(X_test['review'])\n",
    "pred_probs = pipeline.predict_proba(X_test['review'])[:,1]\n",
    "\n",
    "print('예측 정확도는 {0:.4f}'.format(accuracy_score(y_test ,pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e259de63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cool story bro. In what chapter do you shut the fuck up?'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_list2\n",
    "posts[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "22bd2722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cool story bro. In what chapter do you shut the fuck up?\n",
      "turn your wounds into wisdom\n",
      "our love was lost. Your heart turned cold\n",
      "what a job.\n",
      "I love this Pizza, who's the chef?\n",
      "yo Check THIS Out!! Hit me up!! 010-XXXX-XXXX\n",
      "have you heard of Elon Musk's Neural link? That is fucking awesome!!\n",
      "thanks for you help. I really appreciate it\n",
      "I love dogs. They are very cute.\n",
      "I'm broke. Would anyone raise a fund for me? Call me. 010-xxxx-xxxx\n",
      "my life has no hope. I should just go die\n",
      "I hate my mom's beef stew it's fucking disgusting\n",
      "This account is a test account for app development.\n"
     ]
    }
   ],
   "source": [
    "#pipeline.fit(X_train['review'], y_train)\n",
    "for i, post in enumerate(posts):\n",
    "    print(post[-1])\n",
    "    pred =pipeline.predict([post[-1]])\n",
    "    if pred == [0]:\n",
    "        filter_list2[i].append(\"unstable mood\")\n",
    "    #해당 메소드를 실행하면서 학습한 데이터를 기반으로 주어진 Feature가 어떤 확률로 Label로 분류되었는지 계산하여 반환\n",
    "    pipeline.predict_proba([post[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d78b2a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_sentance:  cool story bro. In what chapter do you shut the fuck up? \n",
      " sentiment : [0] \n",
      " percentage: [60.61100304] [%]\n",
      "test_sentance:  turn your wounds into wisdom \n",
      " sentiment : [0] \n",
      " percentage: [54.63695746] [%]\n",
      "test_sentance:  our love was lost. Your heart turned cold \n",
      " sentiment : [0] \n",
      " percentage: [66.33340485] [%]\n",
      "test_sentance:  what a job. \n",
      " sentiment : [0] \n",
      " percentage: [69.13341775] [%]\n",
      "test_sentance:  I love this Pizza, who's the chef? \n",
      " sentiment : [0] \n",
      " percentage: [68.38694565] [%]\n",
      "test_sentance:  yo Check THIS Out!! Hit me up!! 010-XXXX-XXXX \n",
      " sentiment : [0] \n",
      " percentage: [58.78463033] [%]\n",
      "test_sentance:  have you heard of Elon Musk's Neural link? That is fucking awesome!! \n",
      " sentiment : [0] \n",
      " percentage: [69.82025891] [%]\n",
      "test_sentance:  thanks for you help. I really appreciate it \n",
      " sentiment : [0] \n",
      " percentage: [73.33966619] [%]\n",
      "test_sentance:  I love dogs. They are very cute. \n",
      " sentiment : [0] \n",
      " percentage: [56.62183206] [%]\n",
      "test_sentance:  I'm broke. Would anyone raise a fund for me? Call me. 010-xxxx-xxxx \n",
      " sentiment : [0] \n",
      " percentage: [60.1702824] [%]\n",
      "test_sentance:  my life has no hope. I should just go die \n",
      " sentiment : [0] \n",
      " percentage: [58.04113697] [%]\n",
      "test_sentance:  I hate my mom's beef stew it's fucking disgusting \n",
      " sentiment : [0] \n",
      " percentage: [66.43309845] [%]\n",
      "test_sentance:  This account is a test account for app development. \n",
      " sentiment : [0] \n",
      " percentage: [63.12830628] [%]\n"
     ]
    }
   ],
   "source": [
    "# 어떤 확률로 선택했는지 찾는 코드 \n",
    "\n",
    "def ref(x,y):\n",
    "    return x if x>=y else y\n",
    "\n",
    "for post in posts:\n",
    "    percentage1=pipeline.predict_proba([post[-1]])[:,0]\n",
    "    percentage2=pipeline.predict_proba([post[-1]])[:,1]\n",
    "    \n",
    "    print(\"test_sentance: \", post[-1],'\\n',\n",
    "     \"sentiment :\", pred, \"\\n\",\n",
    "      \"percentage:\", ref(percentage1,percentage2)*100,\"[%]\"\n",
    "     )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
